![ACM Research Banner Light](https://github.com/ACM-Research/paperImplementations/assets/108421238/467a89e3-72db-41d7-9a25-51d2c589bfd9)

# qryptik/karthik-sobhirala
## ðŸ“Œ Branch Summary
Implements a BP-RNN, a learnable, unrolled belief-propagation decoder for RLCE collapsed symbols. This model replaces or augments the RS decoder stage on the collapsed length word so the system recovers more correct messages (and provides soft confidence for the CCA recheck)

## ðŸ§  Methodology
### BP-RNN (Belief Propagation Recurrent Neural Network)
Each iteration of the network corresponds to one BP step, using either tied (shared) or untied weights. Inputs are bit-level LLRs derived from the RLCE ciphertext, optionally augmented with syndrome information.

The core of the model is iterative message passing:
* Variable nodes are updated with incoming check messages and channel LLRs using small MLPs or GRU cells.
* Check nodes enforce parity constraints with parameterized functions replacing classical BP operations.
* Messages are scalars or vectors with optional hidden state per edge.

Iterations are unrolled for T steps, with optional residual connections for training stability. Outputs are read out as bit-level LLRs and passed to a Reedâ€“Solomon decoder, optionally with a CCA recheck for verification.

The network is trained end-to-end using cross-entropy loss on simulated RLCE ciphertexts, with regularization (weight decay, dropout) and optimization via Adam. Training data is generated by simulating the RLCE encryption pipeline and introducing sparse errors.

Practical considerations include inference complexity (~O(TÂ·EÂ·h)), quantization for deployment, robustness via varying error patterns, and safety checks via the CCA residual.

In short, the BP-RNN branch provides learnable, iterative decoding that enhances RS+CCA performance while maintaining efficiency and robustness for RLCE ciphertext recovery.
